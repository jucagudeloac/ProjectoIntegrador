{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación del taller 3 Modelo de clasificación de Papers utilizando PySpark\n",
    "\n",
    "Aplicación del taller número 3 del curso ST1800-Almacenamiento y recuperación de la información\n",
    "\n",
    "#### Equipo:\n",
    "\n",
    "1. Juan Carlos Agudelo Acevedo\n",
    "2. Daian Fajardo Becerra\n",
    "3. Hernan Sepulveda Jimenez\n",
    "\n",
    "Profesor: Edwin Nelson Montoya Munera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCIONES LECTURA JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py4j.java_gateway import *\n",
    "port = launch_gateway()\n",
    "gateway = JavaGateway(\n",
    "gateway_parameters=GatewayParameters(port=port),\n",
    "callback_server_parameters=CallbackServerParameters(port=0))\n",
    "random = gateway.jvm.java.util.Random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "\n",
    "def format_name(author):\n",
    "    middle_name = \" \".join(author['middle'])\n",
    "    \n",
    "    if author['middle']:\n",
    "        return \" \".join([author['first'], middle_name, author['last']])\n",
    "    else:\n",
    "        return \" \".join([author['first'], author['last']])\n",
    "\n",
    "\n",
    "def format_affiliation(affiliation):\n",
    "    text = []\n",
    "    location = affiliation.get('location')\n",
    "    if location:\n",
    "        text.extend(list(affiliation['location'].values()))\n",
    "    \n",
    "    institution = affiliation.get('institution')\n",
    "    if institution:\n",
    "        text = [institution] + text\n",
    "    return \", \".join(text)\n",
    "\n",
    "def format_authors(authors, with_affiliation=False):\n",
    "    name_ls = []\n",
    "    \n",
    "    for author in authors:\n",
    "        name = format_name(author)\n",
    "        if with_affiliation:\n",
    "            affiliation = format_affiliation(author['affiliation'])\n",
    "            if affiliation:\n",
    "                name_ls.append(f\"{name} ({affiliation})\")\n",
    "            else:\n",
    "                name_ls.append(name)\n",
    "        else:\n",
    "            name_ls.append(name)\n",
    "    \n",
    "    return \", \".join(name_ls)\n",
    "\n",
    "\n",
    "def format_body(body_text):\n",
    "    texts = [(di['section'], di['text']) for di in body_text]\n",
    "    texts_di = {di['section']: \"\" for di in body_text}\n",
    "    \n",
    "    for section, text in texts:\n",
    "        texts_di[section] += text\n",
    "\n",
    "    body = \"\"\n",
    "\n",
    "    for section, text in texts_di.items():\n",
    "        body += section\n",
    "        body += \"\\n\\n\"\n",
    "        body += text\n",
    "        body += \"\\n\\n\"\n",
    "    \n",
    "    return body\n",
    "\n",
    "def format_bib(bibs):\n",
    "    if type(bibs) == dict:\n",
    "        bibs = list(bibs.values())\n",
    "    bibs = deepcopy(bibs)\n",
    "    formatted = []\n",
    "    \n",
    "    for bib in bibs:\n",
    "        bib['authors'] = format_authors(\n",
    "            bib['authors'], \n",
    "            with_affiliation=False\n",
    "        )\n",
    "        formatted_ls = [str(bib[k]) for k in ['title', 'authors', 'venue', 'year']]\n",
    "        formatted.append(\", \".join(formatted_ls))\n",
    "\n",
    "        \n",
    "    return \"; \".join(formatted)\n",
    "\n",
    "\n",
    "def load_files(dirname):\n",
    "    filenames = os.listdir(dirname)\n",
    "    raw_files = []\n",
    "\n",
    "    for filename in tqdm(filenames):\n",
    "        filename = dirname + filename\n",
    "        file = json.load(open(filename, 'rb'))\n",
    "        raw_files.append(file)\n",
    "    \n",
    "    return raw_files\n",
    "\n",
    "\n",
    "def generate_clean_df(all_files):\n",
    "    cleaned_files = []\n",
    "    \n",
    "    for file in tqdm(all_files):\n",
    "        features = [\n",
    "            file['paper_id'],\n",
    "            file['metadata']['title'],\n",
    "            format_authors(file['metadata']['authors']),\n",
    "            format_authors(file['metadata']['authors'], \n",
    "                           with_affiliation=True),\n",
    "            format_body(file['abstract']),\n",
    "            format_body(file['body_text']),\n",
    "            format_bib(file['bib_entries']),\n",
    "            file['metadata']['authors'],\n",
    "            file['bib_entries']\n",
    "        ]\n",
    "\n",
    "        cleaned_files.append(features)\n",
    "\n",
    "    col_names = ['paper_id', 'title', 'authors',\n",
    "                 'affiliations', 'abstract', 'text', \n",
    "                 'bibliography','raw_authors','raw_bibliography']\n",
    "\n",
    "    clean_df = pd.DataFrame(cleaned_files, columns=col_names)\n",
    "    clean_df = clean_df.drop(['raw_authors', 'raw_bibliography'], axis=1)\n",
    "    clean_df.head()\n",
    "    \n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab3310c7a8e4368bac2a912c0110561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1478.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db096d0056574c89a2ac345e3d2f512c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1478.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Path y lectura de archivos\n",
    "\n",
    "pmc_dir = 'C:/Users/LENOVO/Documents/Maestría/Semestre I/Almacenamiento y Recuperacion de Informacion/Taller3/Script/jason_prueba2/'\n",
    "pmc_files = load_files(pmc_dir)\n",
    "pmc_df = generate_clean_df(pmc_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos la columna de etiqueta\n",
    "pmc_df.insert(7,'tags',\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asignación de etiquetas para los papers por método de frecuencias.\n",
    "\n",
    "Basandonos en un analisis mixto entre palabras obtenidas del LDA, kernels de Kaggle y revisión de algunos Papers destacados, creamos un listado de posibles categorias que aparecen en gran medida en todos los papers y que podrian marcar una diferencia a la hora de buscar un paper de interés.\n",
    "Tomando las etiquetas Transmisión, Incubacion, Riesgo, Infecciones, Origen, Evolución, Vacunas y Supervisión procederemos a elegir las mas representativas según la frecuencia en los papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etiquetas seleccionadas a partir del analisis de los papers\n",
    "possible_tags=['transmission','incubation','risk','infections','origin','evolution','vaccines','surveillance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3ef5f73d1f4bc09de19ca48b11000b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Funcion para asignar la etiqueta con mayor frecuencia por cada paper\n",
    "for i, row in tqdm(pmc_df.iterrows()):\n",
    "    tag={}\n",
    "    for i in possible_tags:\n",
    "        tag[i]=0\n",
    "        #print(i)\n",
    "    for word in possible_tags:\n",
    "        if word in row[\"text\"].lower():\n",
    "            tag[word]=sum(1 for _ in re.finditer(r'\\b%s\\b' % re.escape(word), row[\"text\"],re.IGNORECASE))\n",
    "    tag={k: v for k, v in sorted(tag.items(), key=lambda item: item[1],reverse = True)}\n",
    "    tag={k: tag[k] for k in list(tag)[:1]}\n",
    "    tags=\"\"\n",
    "    for i in tag.items():\n",
    "        if i[1]!=0:\n",
    "            tags+=i[0]+\", \"\n",
    "    tags=tags[:-2]\n",
    "    row['tags']=tags   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>affiliations</th>\n",
       "      <th>abstract</th>\n",
       "      <th>text</th>\n",
       "      <th>bibliography</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000b7d1517ceebb34e1e3e817695b6de03e2fa78</td>\n",
       "      <td>Supplementary Information An eco-epidemiologic...</td>\n",
       "      <td>Julien Mélade, Nicolas Wieseke 4#, Beza Ramazi...</td>\n",
       "      <td>Julien Mélade (2 rue Maxime Rivière, 97490 Sai...</td>\n",
       "      <td></td>\n",
       "      <td>\\n\\n- Figure S1 : Phylogeny of all sequences b...</td>\n",
       "      <td>NDV/HQ266603/Chicken/1992, , , None; MuV/FJ375...</td>\n",
       "      <td>evolution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002e1edfb72cdb4ac11608f785221825d911de83</td>\n",
       "      <td>Profile of Antibodies to the Nucleocapsid Prot...</td>\n",
       "      <td>Xuan Liu, Yulin Shi, Ping Li, Linhai Li, Yanpi...</td>\n",
       "      <td>Xuan Liu (General Hospital of Guangzhou Comman...</td>\n",
       "      <td>Abstract\\n\\nProfiles of antibodies to the nucl...</td>\n",
       "      <td>\\n\\nThe novel severe acute respiratory syndrom...</td>\n",
       "      <td>Identification of a novel coronavirus in patie...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>004f0f8bb66cf446678dc13cf2701feec4f36d76</td>\n",
       "      <td>Healthcare-resource-adjusted vulnerabilities t...</td>\n",
       "      <td>Hanchu Zhou, Jiannan Yang, Kaicheng Tang, † , ...</td>\n",
       "      <td>Hanchu Zhou (City University of Hong Kong, Hon...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction\\n\\nThe 2019-nCoV epidemic has spr...</td>\n",
       "      <td>World Health Organizations. Novel Coronavirus ...</td>\n",
       "      <td>incubation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0092846a890dd9dfd5e55d0731eee0175e485c08</td>\n",
       "      <td>Supplementary appendix</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\\n\\n-Secondary efficacy parameters, by days of...</td>\n",
       "      <td>Design and performance of the CDC real-time re...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00af80743cef9bd8c04c532d74e9c67f0c9312e4</td>\n",
       "      <td>S2 Appendix: ERGM Alternating Graph Statistics</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\\n\\nfor fixed constant weights λ i ≥ 1, (often...</td>\n",
       "      <td>connectivity and degree distributions: Exponen...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   paper_id  \\\n",
       "0  000b7d1517ceebb34e1e3e817695b6de03e2fa78   \n",
       "1  002e1edfb72cdb4ac11608f785221825d911de83   \n",
       "2  004f0f8bb66cf446678dc13cf2701feec4f36d76   \n",
       "3  0092846a890dd9dfd5e55d0731eee0175e485c08   \n",
       "4  00af80743cef9bd8c04c532d74e9c67f0c9312e4   \n",
       "\n",
       "                                               title  \\\n",
       "0  Supplementary Information An eco-epidemiologic...   \n",
       "1  Profile of Antibodies to the Nucleocapsid Prot...   \n",
       "2  Healthcare-resource-adjusted vulnerabilities t...   \n",
       "3                             Supplementary appendix   \n",
       "4     S2 Appendix: ERGM Alternating Graph Statistics   \n",
       "\n",
       "                                             authors  \\\n",
       "0  Julien Mélade, Nicolas Wieseke 4#, Beza Ramazi...   \n",
       "1  Xuan Liu, Yulin Shi, Ping Li, Linhai Li, Yanpi...   \n",
       "2  Hanchu Zhou, Jiannan Yang, Kaicheng Tang, † , ...   \n",
       "3                                                      \n",
       "4                                                      \n",
       "\n",
       "                                        affiliations  \\\n",
       "0  Julien Mélade (2 rue Maxime Rivière, 97490 Sai...   \n",
       "1  Xuan Liu (General Hospital of Guangzhou Comman...   \n",
       "2  Hanchu Zhou (City University of Hong Kong, Hon...   \n",
       "3                                                      \n",
       "4                                                      \n",
       "\n",
       "                                            abstract  \\\n",
       "0                                                      \n",
       "1  Abstract\\n\\nProfiles of antibodies to the nucl...   \n",
       "2                                                      \n",
       "3                                                      \n",
       "4                                                      \n",
       "\n",
       "                                                text  \\\n",
       "0  \\n\\n- Figure S1 : Phylogeny of all sequences b...   \n",
       "1  \\n\\nThe novel severe acute respiratory syndrom...   \n",
       "2  Introduction\\n\\nThe 2019-nCoV epidemic has spr...   \n",
       "3  \\n\\n-Secondary efficacy parameters, by days of...   \n",
       "4  \\n\\nfor fixed constant weights λ i ≥ 1, (often...   \n",
       "\n",
       "                                        bibliography        tags  \n",
       "0  NDV/HQ266603/Chicken/1992, , , None; MuV/FJ375...   evolution  \n",
       "1  Identification of a novel coronavirus in patie...              \n",
       "2  World Health Organizations. Novel Coronavirus ...  incubation  \n",
       "3  Design and performance of the CDC real-time re...              \n",
       "4  connectivity and degree distributions: Exponen...              "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para obtener los papers que contienen exclusivamente alguna etiqueta de las anterior mencionadas\n",
    "\n",
    "def find_papers_from_tags(df,tags):  \n",
    "    new_df=pd.DataFrame(columns=[\"paper_id\",\"title\",\"authors\",\"affiliations\",\"abstract\",\"text\",\"bibliography\",\"tags\"])\n",
    "    tags=tags.split(\",\")\n",
    "    for i, row in tqdm(df.iterrows()):\n",
    "        for tag in tags:\n",
    "            if tag in row['tags']:\n",
    "                new_df=new_df.append(row)\n",
    "                break\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusión de asignación de etiquetas\n",
    "\n",
    "Al comprobar las frecuencias, vemos que las etiquetas Transmisión, Incubación, Riesgo, Infeccion y Vacunas son las más representativas, cabe destacar que 5 categorias para un clasificador multinomial basado en texto representa un gran reto que decidimos tomar pues las etiquetas se solapan en gran medida haciendo asi más dificil el ejercicio posterior de clasificación, este problema puede presentarse en gran medida en la mayoria de maneras de generar las etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9bbc4402f044ac880a03ad9ede47d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Papers Finales\n",
    "new_df=find_papers_from_tags(pmc_df,\"transmission,incubation,risk,infections,vaccines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "risk            47.090663\n",
       "transmission    27.198917\n",
       "infections      17.185386\n",
       "incubation       4.871448\n",
       "vaccines         3.653586\n",
       "Name: tags, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proporcion de etiquetas en el data frame final\n",
    "new_df['tags'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento y Modelo en PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- paper_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- authors: string (nullable = true)\n",
      " |-- affiliations: string (nullable = true)\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- bibliography: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "import re\n",
    "\n",
    "# Carga de funciones escenciales, sc y spark en la sesión \n",
    "sc = SparkContext()\n",
    "spark = SparkSession(sc)\n",
    "sql = SQLContext(sc)\n",
    "\n",
    "# Convierto el dataframe pandas a dataframe Spark (rdd)\n",
    "spark_df = sql.createDataFrame(new_df)\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Etapa de Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+\n",
      "|            paper_id|        tags|              tokens|\n",
      "+--------------------+------------+--------------------+\n",
      "|004f0f8bb66cf4466...|  incubation|[introduction, , ...|\n",
      "|00c75478b9f6b815f...|        risk|[dear, editor:, ,...|\n",
      "|00fddd1ce0dae8535...|        risk|[, , i, n, the, i...|\n",
      "|01686b614a614913b...|        risk|[aerosols, , sali...|\n",
      "|01a5049f7f6965eac...|transmission|[dear, editor, , ...|\n",
      "|022fbc7bea2e25340...|        risk|[correspondence, ...|\n",
      "|023782486e5eef9d9...|  infections|[robert, walgate,...|\n",
      "|0244a8c0153dcbe7d...|transmission|[, , east, respir...|\n",
      "|024b30561568979f5...|  infections|[introduction, , ...|\n",
      "|028a4948d8e10f9ec...|        risk|[jaad, online:, n...|\n",
      "|02c2c01e1908658a0...|        risk|[contents, lists,...|\n",
      "|03345c814dbe72221...|transmission|[sir, model, deta...|\n",
      "|0401e2a525cc6eeb5...|        risk|[, , authors, hav...|\n",
      "|044d1e54d0a62dcd6...|        risk|[contents, lists,...|\n",
      "|04bf954dd55e2ee7e...|    vaccines|[letters, to, the...|\n",
      "|04ebfb2287e60db45...|        risk|[effect, of, the,...|\n",
      "|04f1daa538285f334...|transmission|[title, page, , t...|\n",
      "|050086847f6644a31...|        risk|[to, the, editor:...|\n",
      "|05389a55ca4f5de2b...|        risk|[sir, , when, con...|\n",
      "|054bae9b7f743ea7c...|transmission|[, , number, of, ...|\n",
      "+--------------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Tokenizacion\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenization=Tokenizer(inputCol='text',outputCol='tokens')\n",
    "tokenized_df=tokenization.transform(spark_df)\n",
    "#tokenized_df.select('tokens').show(1,False)\n",
    "tokenized_df.select(['paper_id','tags','tokens']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remoción de Stopwords  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+\n",
      "|            paper_id|        tags|      refined_tokens|\n",
      "+--------------------+------------+--------------------+\n",
      "|004f0f8bb66cf4466...|  incubation|[introduction, , ...|\n",
      "|00c75478b9f6b815f...|        risk|[dear, editor:, ,...|\n",
      "|00fddd1ce0dae8535...|        risk|[, , n, islamic, ...|\n",
      "|01686b614a614913b...|        risk|[aerosols, , sali...|\n",
      "|01a5049f7f6965eac...|transmission|[dear, editor, , ...|\n",
      "|022fbc7bea2e25340...|        risk|[correspondence, ...|\n",
      "|023782486e5eef9d9...|  infections|[robert, walgate,...|\n",
      "|0244a8c0153dcbe7d...|transmission|[, , east, respir...|\n",
      "|024b30561568979f5...|  infections|[introduction, , ...|\n",
      "|028a4948d8e10f9ec...|        risk|[jaad, online:, n...|\n",
      "|02c2c01e1908658a0...|        risk|[contents, lists,...|\n",
      "|03345c814dbe72221...|transmission|[sir, model, deta...|\n",
      "|0401e2a525cc6eeb5...|        risk|[, , authors, not...|\n",
      "|044d1e54d0a62dcd6...|        risk|[contents, lists,...|\n",
      "|04bf954dd55e2ee7e...|    vaccines|[letters, editor,...|\n",
      "|04ebfb2287e60db45...|        risk|[effect, covid-19...|\n",
      "|04f1daa538285f334...|transmission|[title, page, , t...|\n",
      "|050086847f6644a31...|        risk|[editor:, , patie...|\n",
      "|05389a55ca4f5de2b...|        risk|[sir, , consideri...|\n",
      "|054bae9b7f743ea7c...|transmission|[, , number, pati...|\n",
      "+--------------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Remoción de Stopwords\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "stopword_removal=StopWordsRemover(inputCol='tokens',outputCol='refined_tokens')\n",
    "refined_df=stopword_removal.transform(tokenized_df)\n",
    "#refined_df2 = refined_df.select(['tokens','refined_tokens'])\n",
    "refined_df2 = refined_df.select(['paper_id','tags','refined_tokens'])\n",
    "#refined_df2.printSchema()\n",
    "refined_df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Etapa de Lematización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+--------------------+\n",
      "|            paper_id|        tags|      refined_tokens|        tokens_lemma|\n",
      "+--------------------+------------+--------------------+--------------------+\n",
      "|004f0f8bb66cf4466...|  incubation|[introduction, , ...|[introduction, , ...|\n",
      "|00c75478b9f6b815f...|        risk|[dear, editor:, ,...|[dear, editor:, ,...|\n",
      "|00fddd1ce0dae8535...|        risk|[, , n, islamic, ...|[, , n, islamic, ...|\n",
      "|01686b614a614913b...|        risk|[aerosols, , sali...|[aerosol, , saliv...|\n",
      "|01a5049f7f6965eac...|transmission|[dear, editor, , ...|[dear, editor, , ...|\n",
      "|022fbc7bea2e25340...|        risk|[correspondence, ...|[correspondence, ...|\n",
      "|023782486e5eef9d9...|  infections|[robert, walgate,...|[robert, walgate,...|\n",
      "|0244a8c0153dcbe7d...|transmission|[, , east, respir...|[, , east, respir...|\n",
      "|024b30561568979f5...|  infections|[introduction, , ...|[introduction, , ...|\n",
      "|028a4948d8e10f9ec...|        risk|[jaad, online:, n...|[jaad, online:, n...|\n",
      "|02c2c01e1908658a0...|        risk|[contents, lists,...|[content, list, a...|\n",
      "|03345c814dbe72221...|transmission|[sir, model, deta...|[sir, model, deta...|\n",
      "|0401e2a525cc6eeb5...|        risk|[, , authors, not...|[, , author, noth...|\n",
      "|044d1e54d0a62dcd6...|        risk|[contents, lists,...|[content, list, a...|\n",
      "|04bf954dd55e2ee7e...|    vaccines|[letters, editor,...|[letter, editor, ...|\n",
      "|04ebfb2287e60db45...|        risk|[effect, covid-19...|[effect, covid-19...|\n",
      "|04f1daa538285f334...|transmission|[title, page, , t...|[title, page, , t...|\n",
      "|050086847f6644a31...|        risk|[editor:, , patie...|[editor:, , patie...|\n",
      "|05389a55ca4f5de2b...|        risk|[sir, , consideri...|[sir, , consideri...|\n",
      "|054bae9b7f743ea7c...|transmission|[, , number, pati...|[, , number, pati...|\n",
      "+--------------------+------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lematizacion\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType, IntegerType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "lemma_udf = udf(lambda row: [wordnet_lemmatizer.lemmatize(w) for w in row], ArrayType(StringType()))\n",
    "\n",
    "lemma_df = refined_df2.withColumn('tokens_lemma', lemma_udf(col('refined_tokens')))\n",
    "refined_df3 = lemma_df.select(['paper_id','tags','refined_tokens','tokens_lemma'])\n",
    "#refined_df3.select(['tokens_lemma']).show(1,False)\n",
    "refined_df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remoción de caracteres especiales (no alfanumericos) y palabras con longitud menos a 3 caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+\n",
      "|            paper_id|        tags|     cleaned2_tokens|\n",
      "+--------------------+------------+--------------------+\n",
      "|004f0f8bb66cf4466...|  incubation|[introduction, , ...|\n",
      "|00c75478b9f6b815f...|        risk|[dear, editor, , ...|\n",
      "|00fddd1ce0dae8535...|        risk|[, , n, islamic, ...|\n",
      "|01686b614a614913b...|        risk|[aerosol, , saliv...|\n",
      "|01a5049f7f6965eac...|transmission|[dear, editor, , ...|\n",
      "|022fbc7bea2e25340...|        risk|[correspondence, ...|\n",
      "|023782486e5eef9d9...|  infections|[robert, walgate,...|\n",
      "|0244a8c0153dcbe7d...|transmission|[, , east, respir...|\n",
      "|024b30561568979f5...|  infections|[introduction, , ...|\n",
      "|028a4948d8e10f9ec...|        risk|[jaad, online, no...|\n",
      "|02c2c01e1908658a0...|        risk|[content, list, a...|\n",
      "|03345c814dbe72221...|transmission|[sir, model, deta...|\n",
      "|0401e2a525cc6eeb5...|        risk|[, , author, noth...|\n",
      "|044d1e54d0a62dcd6...|        risk|[content, list, a...|\n",
      "|04bf954dd55e2ee7e...|    vaccines|[letter, editor, ...|\n",
      "|04ebfb2287e60db45...|        risk|[effect, covid19,...|\n",
      "|04f1daa538285f334...|transmission|[title, page, , t...|\n",
      "|050086847f6644a31...|        risk|[editor, , patien...|\n",
      "|05389a55ca4f5de2b...|        risk|[sir, , consideri...|\n",
      "|054bae9b7f743ea7c...|transmission|[, , number, pati...|\n",
      "+--------------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remoción de tokens con menos de 3 caracteres\n",
    "cleaning_udf = udf(lambda row: [w for w in row in len(w)>2], ArrayType(StringType()))\n",
    "\n",
    "clean_df = refined_df3.withColumn('cleaned_tokens', lemma_udf(col('tokens_lemma')))\n",
    "refined_df4 = clean_df.select(['paper_id','tags','refined_tokens','cleaned_tokens'])\n",
    "#refined_df4.show()\n",
    "\n",
    "# Remocion de caracteres no alfanumericos\n",
    "spec_char_udf = udf(lambda row: [re.sub(r'[^A-Za-z0-9]+','',w) for w in row], ArrayType(StringType()))\n",
    "\n",
    "clean_df2 = refined_df4.withColumn('cleaned2_tokens', spec_char_udf(col('cleaned_tokens')))\n",
    "refined_df5 = clean_df2.select(['paper_id','tags','cleaned2_tokens'])\n",
    "refined_df5.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conteo de tokens con CountVectorizer y CountVectorizerModel \n",
    "\n",
    "Esta función la utilizalamos para ayudar a convertir una colección de documentos de texto en vectores de **conteos** de tokens.\n",
    "Cuando un diccionario apriori no está disponible, CountVectorizer puede usarse como un estimator para extraer \n",
    "el vocabulario y generar el modelo CountVectorizerModel nos permitirá ver las representaciones dispersas para los documentos \n",
    "sobre el vocabulario, que luego se pueden pasar a otros algoritmos, adicionalmente con el parametro minDF en 3 aprovechamos que las palabras que no aparecen ni en un 3% en los textos los descartaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+\n",
      "|     cleaned2_tokens|        tags|     countVectotizer|\n",
      "+--------------------+------------+--------------------+\n",
      "|[introduction, , ...|  incubation|(7605,[0,3,4,7,11...|\n",
      "|[dear, editor, , ...|        risk|(7605,[0,1,2,4,6,...|\n",
      "|[, , n, islamic, ...|        risk|(7605,[0,1,2,4,5,...|\n",
      "|[aerosol, , saliv...|        risk|(7605,[0,1,2,6,7,...|\n",
      "|[dear, editor, , ...|transmission|(7605,[0,2,6,8,9,...|\n",
      "|[correspondence, ...|        risk|(7605,[0,1,2,4,5,...|\n",
      "|[robert, walgate,...|  infections|(7605,[0,1,4,5,7,...|\n",
      "|[, , east, respir...|transmission|(7605,[0,2,4,5,6,...|\n",
      "|[introduction, , ...|  infections|(7605,[0,2,3,4,5,...|\n",
      "|[jaad, online, no...|        risk|(7605,[0,1,2,3,4,...|\n",
      "+--------------------+------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count Vectorizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"cleaned2_tokens\", outputCol=\"countVectotizer\", minDF = 3)\n",
    "\n",
    "tokens_dep = refined_df5.select(['cleaned2_tokens','tags'])\n",
    "modelCountVectorizer = cv.fit(tokens_dep)\n",
    "result = modelCountVectorizer.transform(tokens_dep)\n",
    "refined_df6 = result.select(['tags','cleaned2_tokens','countVectotizer'])\n",
    "result.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+--------------------+--------------------+\n",
      "|     cleaned2_tokens|        tags|     countVectotizer|         rawFeatures|            features|\n",
      "+--------------------+------------+--------------------+--------------------+--------------------+\n",
      "|[introduction, , ...|  incubation|(7605,[0,3,4,7,11...|(262144,[836,1769...|(262144,[836,1769...|\n",
      "|[dear, editor, , ...|        risk|(7605,[0,1,2,4,6,...|(262144,[666,1769...|(262144,[666,1769...|\n",
      "|[, , n, islamic, ...|        risk|(7605,[0,1,2,4,5,...|(262144,[687,2472...|(262144,[687,2472...|\n",
      "|[aerosol, , saliv...|        risk|(7605,[0,1,2,6,7,...|(262144,[14,666,3...|(262144,[14,666,3...|\n",
      "|[dear, editor, , ...|transmission|(7605,[0,2,6,8,9,...|(262144,[1731,176...|(262144,[1731,176...|\n",
      "|[correspondence, ...|        risk|(7605,[0,1,2,4,5,...|(262144,[813,2710...|(262144,[813,2710...|\n",
      "|[robert, walgate,...|  infections|(7605,[0,1,4,5,7,...|(262144,[666,1156...|(262144,[666,1156...|\n",
      "|[, , east, respir...|transmission|(7605,[0,2,4,5,6,...|(262144,[2938,302...|(262144,[2938,302...|\n",
      "|[introduction, , ...|  infections|(7605,[0,2,3,4,5,...|(262144,[590,1330...|(262144,[590,1330...|\n",
      "|[jaad, online, no...|        risk|(7605,[0,1,2,3,4,...|(262144,[2326,252...|(262144,[2326,252...|\n",
      "+--------------------+------------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TF - IDF\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"cleaned2_tokens\", outputCol=\"rawFeatures\")\n",
    "featurizedData = hashingTF.transform(result)\n",
    "#featurizedData.show(10)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "rescaledData.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "Word2Vec al ser un estimador que toma secuencias de palabras que representan documentos, con la funcion Word2VecModel entrenamos un modelo que asigna cada palabra a un vector único de tamaño fijo utilizando el promedio de todas las palabras en el documento, este vector se puede usar como características para predicciones o cálculos de similitud de documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+--------------------+\n",
      "|     cleaned2_tokens|        tags|     countVectotizer|            word2Vec|\n",
      "+--------------------+------------+--------------------+--------------------+\n",
      "|[introduction, , ...|  incubation|(7605,[0,3,4,7,11...|[0.22456058674946...|\n",
      "|[dear, editor, , ...|        risk|(7605,[0,1,2,4,6,...|[0.20205751460577...|\n",
      "|[, , n, islamic, ...|        risk|(7605,[0,1,2,4,5,...|[0.22655495013870...|\n",
      "|[aerosol, , saliv...|        risk|(7605,[0,1,2,6,7,...|[0.15916733699970...|\n",
      "|[dear, editor, , ...|transmission|(7605,[0,2,6,8,9,...|[0.19882603538939...|\n",
      "|[correspondence, ...|        risk|(7605,[0,1,2,4,5,...|[0.23686730223547...|\n",
      "|[robert, walgate,...|  infections|(7605,[0,1,4,5,7,...|[0.20553478638240...|\n",
      "|[, , east, respir...|transmission|(7605,[0,2,4,5,6,...|[0.19253316088884...|\n",
      "|[introduction, , ...|  infections|(7605,[0,2,3,4,5,...|[0.22448527269232...|\n",
      "|[jaad, online, no...|        risk|(7605,[0,1,2,3,4,...|[0.20141979732099...|\n",
      "+--------------------+------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------+--------------------+\n",
      "|         word|              vector|\n",
      "+-------------+--------------------+\n",
      "| laryngoscope|[0.19839483499526...|\n",
      "|    pathogens|[0.24059724807739...|\n",
      "|  zootechnics|[0.05649237707257...|\n",
      "|antithymocyte|[0.18657037615776...|\n",
      "|     incident|[0.12368261069059...|\n",
      "|   matrixwave|[0.08848483860492...|\n",
      "|       lauryl|[-0.0146178770810...|\n",
      "|      serious|[0.65047770738601...|\n",
      "|        brink|[-0.1677145957946...|\n",
      "|   pyraloidea|[-0.0212397314608...|\n",
      "+-------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Word2vec (Opcional)\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"cleaned2_tokens\", outputCol=\"word2Vec\")\n",
    "modelWord2Vec = word2Vec.fit(result)\n",
    "result2 = modelWord2Vec.transform(result)\n",
    "result2.show(10)\n",
    "\n",
    "Vectors = modelWord2Vec.getVectors()\n",
    "Vectors.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos de Clasificación\n",
    "\n",
    "Para este problema de clasificación multinomial en donde buscamos identificar la categoria a la que pertenecen los Papers sobre el covid y asi simplificar las busquedas que usuarios esten interesados en encontrar, colocamos a competir dos algoritmos de clasificación, un Nayve Bayer y un Random Forest los cuales tienen una naturaleza muy diferentes pues mientras uno utiliza probabilidades condicionales a través de las transformaciones previas de los textos el otro tiene una arquitectura de arboles los cuales se rigen bajo impureza y criterios como el de Gini para finalmente tomar un promedio de arboles y tener estimaciones de las probabilidades para cada categoria que asignamos en cada Paper para los conjuntos training y test.\n",
    "\n",
    "Reuniendo el preprocesamiento en un pipeline bajo la libreria Pipeline, unimos los features finales que serviran para la clasificación tomando un muestreo (80%/20%) respectivamente para entrenamiento y evaluacion. Dadas las limitaciones en procesamiento, entrenamos con una muestra de 1478 papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|            paper_id|        tags|     cleaned2_tokens|     countVectotizer|         rawFeatures|            features|label|\n",
      "+--------------------+------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|004f0f8bb66cf4466...|  incubation|[introduction, , ...|(7605,[0,3,4,7,11...|(262144,[836,1769...|(262144,[836,1769...|  3.0|\n",
      "|00c75478b9f6b815f...|        risk|[dear, editor, , ...|(7605,[0,1,2,4,6,...|(262144,[666,1769...|(262144,[666,1769...|  0.0|\n",
      "|00fddd1ce0dae8535...|        risk|[, , n, islamic, ...|(7605,[0,1,2,4,5,...|(262144,[687,2472...|(262144,[687,2472...|  0.0|\n",
      "|01686b614a614913b...|        risk|[aerosol, , saliv...|(7605,[0,1,2,6,7,...|(262144,[14,666,3...|(262144,[14,666,3...|  0.0|\n",
      "|01a5049f7f6965eac...|transmission|[dear, editor, , ...|(7605,[0,2,6,8,9,...|(262144,[1731,176...|(262144,[1731,176...|  1.0|\n",
      "+--------------------+------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Modelo de clasificación\n",
    "\n",
    "# Indices\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "label_stringIdx = StringIndexer(inputCol = \"tags\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[cv,hashingTF,idf,label_stringIdx])\n",
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(refined_df5)\n",
    "refined_df6 = pipelineFit.transform(refined_df5)\n",
    "refined_df6.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semilla para reproducir\n",
    "#trainingData = refined_df6\n",
    "(trainingData, testData) = refined_df6.randomSplit([0.7, 0.3], seed = 1000)\n",
    "#print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "#print(\"Test Dataset Count: \" + str(testData.count()))\n",
    "\n",
    "#trainingData.printSchema()\n",
    "\n",
    "#refined_df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------------------------------+------------+------------------------------+-----+----------+\n",
      "|                      paper_id|               cleaned2_tokens|        tags|                   probability|label|prediction|\n",
      "+------------------------------+------------------------------+------------+------------------------------+-----+----------+\n",
      "|a81191091dd2de0cfa93a7869b6...|[, , address, need, populat...|  infections|[1.0,5.821152268582547E-17,...|  2.0|       0.0|\n",
      "|9ede8bf72b414ce35b05828c6ed...|[, , infectious, disease, o...|  incubation|[1.0,5.0289789898927E-18,1....|  3.0|       0.0|\n",
      "|1bed4f8a5412cfa315b0936705a...|[, , virus, positive, sense...|transmission|[1.0,2.269125438030524E-21,...|  1.0|       0.0|\n",
      "|f4a58859fbef59081909da083e0...|[, , dear, editor, read, gr...|  incubation|[1.0,1.318608689196907E-21,...|  3.0|       0.0|\n",
      "|10c8214c3c721e2a56dae312e8e...|[inuit, community, beat, co...|        risk|[1.0,1.2184645096461142E-22...|  0.0|       0.0|\n",
      "|88449101d4c40d01dc551a4a46f...|[background, , , half, noro...|        risk|[1.0,6.289484676627608E-23,...|  0.0|       0.0|\n",
      "|76a0a451776ae8450bd251395fb...|[clipboard, , severe, acute...|transmission|[1.0,2.13420116722298E-25,5...|  1.0|       0.0|\n",
      "|20b53d489f1e84d5ad1701b4165...|[, , outbreak, involve, tes...|        risk|[1.0,5.291576085842784E-26,...|  0.0|       0.0|\n",
      "|42bd49d985c6be5e102939ce99b...|[, , epidemic, coronavirus,...|transmission|[1.0,2.188927173875536E-27,...|  1.0|       0.0|\n",
      "|6fc30224f0fef04b0cb8feab92a...|[patient, transport, operat...|        risk|[1.0,7.344042732640773E-31,...|  0.0|       0.0|\n",
      "+------------------------------+------------------------------+------------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4247730395079629"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nayve Bayes\n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Modelo de entrenamiento\n",
    "nb = NaiveBayes(smoothing=1)\n",
    "model2 = nb.fit(trainingData)\n",
    "predictions2 = model2.transform(testData)\n",
    "predictions2.filter(predictions2['prediction'] == 0).select(\"paper_id\",\"cleaned2_tokens\",\"tags\",\"probability\",\"label\",\"prediction\").orderBy(\"probability\", ascending=False).show(n = 10, truncate = 30)\n",
    "\n",
    "# Evaluador de precisión de modelo\n",
    "evaluator2 = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator2.evaluate(predictions2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------------------------------+----+------------------------------+-----+----------+\n",
      "|                      paper_id|               cleaned2_tokens|tags|                   probability|label|prediction|\n",
      "+------------------------------+------------------------------+----+------------------------------+-----+----------+\n",
      "|5238852a9879f7f06255ffbeb5a...|[sir, , time, writing, lett...|risk|[0.5455564037754126,0.24373...|  0.0|       0.0|\n",
      "|e4f808dcd1e97c1941d3709a2b6...|[, , global, pandemic, covi...|risk|[0.5430719666341247,0.24314...|  0.0|       0.0|\n",
      "|ff4734ba10d0f14431cfc028cbd...|[, , , responsible, covid19...|risk|[0.5385969930716061,0.25131...|  0.0|       0.0|\n",
      "|94b76eafee4f06be376edc6d432...|[correspondence, , reply, d...|risk|[0.5378213485349316,0.24674...|  0.0|       0.0|\n",
      "|27e8e8746befaf798daed5fc73a...|[, , covid19, rapidly, expa...|risk|[0.5346771689334063,0.25514...|  0.0|       0.0|\n",
      "|82945a2379f35c0c5f7b6d6a6a2...|[, , date, coronavirus, sar...|risk|[0.5329856522473859,0.24804...|  0.0|       0.0|\n",
      "|022fbc7bea2e253407b6abea4f5...|[correspondence, 390, , www...|risk|[0.531977323785885,0.256143...|  0.0|       0.0|\n",
      "|25a77fd1e417ea69da63fd1a6a0...|[47, , fatality, likely, re...|risk|[0.5315142642684401,0.25542...|  0.0|       0.0|\n",
      "|028a4948d8e10f9ece7de820e4f...|[jaad, online, note, , comm...|risk|[0.5314708310875217,0.25363...|  0.0|       0.0|\n",
      "|3cfeee9ce10faf5e6c0ec489794...|[dear, editor, , corona, vi...|risk|[0.5301946025438667,0.25705...|  0.0|       0.0|\n",
      "+------------------------------+------------------------------+----+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.26035920027109455"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RF\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"rawFeatures\", numTrees = 100, maxDepth = 4, maxBins = 32)\n",
    "\n",
    "# Modelo de entrenamiento\n",
    "rfModel = rf.fit(trainingData)\n",
    "predictions = rfModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"paper_id\",\"cleaned2_tokens\",\"tags\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)\n",
    "\n",
    "# Evaluador de precisión de modelo\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evaluar los resultados utilizamos el método MulticlassClassificationEvaluator, el cual captura una metrica entre la precisión y el recall pero que no nos muestra valores explicitos para cada valor tenido en cuenta en la metrica, simplemente evalua las salidas del modelo con las etiquetas respectivas en el conjunto de Test, medidas como el ROC son algo mas tediosas de construir en Spark en comparación a R o Python.\n",
    "Tomando como mejor clasificador el Nayve Bayes, cabe destacar que los modelos de clasificación de textos cuando se trata de libros o papers extensos suelen tener una precision media baja en general y la espectativa de tener precisiones mayores al 70% como en clasificaciones de features menos dispersas es reduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación de nuevos textos\n",
    "\n",
    "Tomamos una muestra de 2758 papers completamente nuevos 7 aplicamos el modelo 2 (Nayve Bayes), cabe resaltar un detalle en el preprocesamiento de los nuevos textos, el cual es un limitante de la libreria en la que se realizó el clasificador pues al comparar otras librerias como Caret de R o SciKit Learn de Python, estos ya tienen en cuenta las funciones de preprocesamiento realizadas y se aplican directamente a los textos, procedemos a realizar el preprocesamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56cde99be0a64c5383ba5c4df8bc9cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2759.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82494d9a4f3b45f3813a3b96c4fc909a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2759.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Path y lectura de archivos\n",
    "\n",
    "dir_new = 'C:/Users/LENOVO/Documents/Maestría/Semestre I/Almacenamiento y Recuperacion de Informacion/Taller3/Script/papers_nuevos/'\n",
    "files_news = load_files(dir_new)\n",
    "df_news = generate_clean_df(files_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_new = sql.createDataFrame(df_news)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization_new=Tokenizer(inputCol='text',outputCol='tokens')\n",
    "tokenized_df_new=tokenization_new.transform(spark_df_new)\n",
    "\n",
    "stopword_removal_new=StopWordsRemover(inputCol='tokens',outputCol='refined_tokens')\n",
    "refined_df2_new=stopword_removal_new.transform(tokenized_df_new)\n",
    "\n",
    "lemma_df_new = refined_df2_new.withColumn('tokens_lemma', lemma_udf(col('refined_tokens')))\n",
    "refined_df3_new = lemma_df_new.select(['paper_id','refined_tokens','tokens_lemma'])\n",
    "\n",
    "clean_df_new = refined_df3_new.withColumn('cleaned_tokens', lemma_udf(col('tokens_lemma')))\n",
    "refined_df4_new = clean_df_new.select(['paper_id','refined_tokens','cleaned_tokens'])\n",
    "\n",
    "clean_df2_new = refined_df4.withColumn('cleaned2_tokens', spec_char_udf(col('cleaned_tokens')))\n",
    "refined_df5_new = clean_df2_new.select(['paper_id','cleaned2_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_new = CountVectorizer(inputCol=\"cleaned2_tokens\", outputCol=\"countVectotizer\", minDF = 3)\n",
    "\n",
    "tokens_dep_new = refined_df5_new.select(['cleaned2_tokens'])\n",
    "modelCountVectorizer_new = cv_new.fit(tokens_dep_new)\n",
    "result_new = modelCountVectorizer_new.transform(tokens_dep_new)\n",
    "refined_df6_new = result_new.select(['cleaned2_tokens','countVectotizer'])\n",
    "\n",
    "hashingTF_new = HashingTF(inputCol=\"cleaned2_tokens\", outputCol=\"rawFeatures\")\n",
    "featurizedData_new = hashingTF_new.transform(result_new)\n",
    "\n",
    "idf_new = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel_new = idf_new.fit(featurizedData_new)\n",
    "rescaledData_new = idfModel_new.transform(featurizedData_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|            paper_id|     cleaned2_tokens|     countVectotizer|         rawFeatures|            features|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|004f0f8bb66cf4466...|[introduction, , ...|(7605,[0,3,4,7,11...|(262144,[836,1769...|(262144,[836,1769...|\n",
      "|00c75478b9f6b815f...|[dear, editor, , ...|(7605,[0,1,2,4,6,...|(262144,[666,1769...|(262144,[666,1769...|\n",
      "|00fddd1ce0dae8535...|[, , n, islamic, ...|(7605,[0,1,2,4,5,...|(262144,[687,2472...|(262144,[687,2472...|\n",
      "|01686b614a614913b...|[aerosol, , saliv...|(7605,[0,1,2,6,7,...|(262144,[14,666,3...|(262144,[14,666,3...|\n",
      "|01a5049f7f6965eac...|[dear, editor, , ...|(7605,[0,2,6,8,9,...|(262144,[1731,176...|(262144,[1731,176...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_new = Pipeline(stages=[cv_new,hashingTF_new,idf_new])\n",
    "\n",
    "pipelineFit_new = pipeline_new.fit(refined_df5_new)\n",
    "refined_df6_new = pipelineFit_new.transform(refined_df5_new)\n",
    "refined_df6_new.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_new = model2.transform(refined_df6_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Papers nuevos con su etiqueta respectiva basadas en el modelo de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+\n",
      "|            paper_id|            features|         probability|prediction|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "|004f0f8bb66cf4466...|(262144,[836,1769...|[1.0,3.0644112888...|       0.0|\n",
      "|00c75478b9f6b815f...|(262144,[666,1769...|[1.0,0.0,0.0,0.0,...|       0.0|\n",
      "|00fddd1ce0dae8535...|(262144,[687,2472...|[1.0,2.3303642305...|       0.0|\n",
      "|01686b614a614913b...|(262144,[14,666,3...|[1.0,0.0,0.0,0.0,...|       0.0|\n",
      "|01a5049f7f6965eac...|(262144,[1731,176...|[1.38434738329597...|       1.0|\n",
      "|022fbc7bea2e25340...|(262144,[813,2710...|[1.0,8.5831504908...|       0.0|\n",
      "|023782486e5eef9d9...|(262144,[666,1156...|[2.83591685679528...|       2.0|\n",
      "|0244a8c0153dcbe7d...|(262144,[2938,302...|[3.48407404034288...|       1.0|\n",
      "|024b30561568979f5...|(262144,[590,1330...|[1.0,1.1289964771...|       0.0|\n",
      "|028a4948d8e10f9ec...|(262144,[2326,252...|[1.0,1.1338959728...|       0.0|\n",
      "|02c2c01e1908658a0...|(262144,[511,632,...|[1.0,8.7292261235...|       0.0|\n",
      "|03345c814dbe72221...|(262144,[666,1707...|[1.93650961303564...|       1.0|\n",
      "|0401e2a525cc6eeb5...|(262144,[666,1251...|[1.0,6.2802878953...|       0.0|\n",
      "|044d1e54d0a62dcd6...|(262144,[63,539,1...|[1.0,2.3694147374...|       0.0|\n",
      "|04bf954dd55e2ee7e...|(262144,[666,2050...|[1.0,5.6972905686...|       0.0|\n",
      "|04ebfb2287e60db45...|(262144,[666,5647...|[1.0,8.1352532845...|       0.0|\n",
      "|04f1daa538285f334...|(262144,[2437,337...|[1.0,1.5020994862...|       0.0|\n",
      "|050086847f6644a31...|(262144,[19329,19...|[1.0,3.8163726297...|       0.0|\n",
      "|05389a55ca4f5de2b...|(262144,[1508,581...|[2.86065181542404...|       1.0|\n",
      "|054bae9b7f743ea7c...|(262144,[666,1769...|[3.04498562033245...|       1.0|\n",
      "|0568178a56eac6183...|(262144,[14,353,1...|[1.0,4.0964707360...|       0.0|\n",
      "|0611763ce6ada52bc...|(262144,[619,666,...|[6.14344636544518...|       1.0|\n",
      "|067207286961125e2...|(262144,[1769,374...|[1.0,5.9137189266...|       0.0|\n",
      "|06799f68d32524cee...|(262144,[836,1667...|[2.83960704810456...|       1.0|\n",
      "|06ae3906fb8f239bc...|(262144,[1363,176...|[1.0,2.9220694015...|       0.0|\n",
      "|075679ca83a0a13d0...|(262144,[162,836,...|[1.0,0.0,0.0,0.0,...|       0.0|\n",
      "|0760e79585cd85c7e...|(262144,[1587,176...|[1.0,3.3983719752...|       0.0|\n",
      "|076d25a4a901708a3...|(262144,[619,1251...|[1.0,0.0,0.0,0.0,...|       0.0|\n",
      "|0782ca6248d0429ec...|(262144,[836,1466...|[1.0,0.0,0.0,0.0,...|       0.0|\n",
      "|07849c55a82b791e3...|(262144,[7717,972...|[4.21259780670505...|       4.0|\n",
      "|08339aa5e21ebd96b...|(262144,[2424,278...|[1.0,0.0,0.0,0.0,...|       0.0|\n",
      "|083b2933b78310531...|(262144,[666,1769...|[1.0,0.0,0.0,0.0,...|       0.0|\n",
      "|0843b669142963c77...|(262144,[1739,559...|[1.0,1.8670152171...|       0.0|\n",
      "|08c0f65de431fceb0...|(262144,[666,3298...|[1.0,3.9199142269...|       0.0|\n",
      "|08db45ce51ed0b3b3...|(262144,[666,1769...|[1.0,0.0,0.0,0.0,...|       0.0|\n",
      "|08f404906bd45b2f7...|(262144,[666,1769...|[3.10507379206924...|       1.0|\n",
      "|0916d018d0fa000a7...|(262144,[514,1846...|[8.58882977131940...|       2.0|\n",
      "|092afdfbe511b5a80...|(262144,[3280,380...|[1.0,2.2311992267...|       0.0|\n",
      "|092b943b322d66f91...|(262144,[1769,184...|[1.0,8.0200046462...|       0.0|\n",
      "|0935e488deae02c9d...|(262144,[666,1707...|[1.0,0.0,0.0,0.0,...|       0.0|\n",
      "|097d0393351307300...|(262144,[666,2326...|[1.0,0.0,0.0,0.0,...|       0.0|\n",
      "|09da1f7c0df0de6af...|(262144,[666,2326...|[1.0,6.5701520378...|       0.0|\n",
      "|0a135787cd841ddec...|(262144,[3666,422...|[1.96327883242102...|       2.0|\n",
      "|0a1beb9a503c932f4...|(262144,[666,4177...|[1.0,3.9888201207...|       0.0|\n",
      "|0a788544235f577b6...|(262144,[6256,649...|[0.0,8.2637569596...|       2.0|\n",
      "|0b06b792ce1792bbe...|(262144,[1598,604...|[2.08636841757492...|       1.0|\n",
      "|0b52a2320b9a347ba...|(262144,[666,1836...|[1.0,0.0,0.0,0.0,...|       0.0|\n",
      "|0bed614165ea5e8b2...|(262144,[1769,278...|[8.47139009810862...|       1.0|\n",
      "|0c8febdb69cd99fd7...|(262144,[59,167,6...|[1.0,0.0,0.0,0.0,...|       0.0|\n",
      "|0cc83e2cace72ea04...|(262144,[2596,333...|[1.0,0.0,0.0,0.0,...|       0.0|\n",
      "|0d000d65c859391a8...|(262144,[5595,955...|[1.0,0.0,0.0,0.0,...|       0.0|\n",
      "|0d6a0014b5c8b9994...|(262144,[1839,262...|[1.0,0.0,0.0,0.0,...|       0.0|\n",
      "|0d8679c80db86bee1...|(262144,[329,666,...|[9.69396683776218...|       1.0|\n",
      "|0da4259e7738a5ef4...|(262144,[1439,176...|[1.0,0.0,0.0,0.0,...|       0.0|\n",
      "|0ddcca2498c941122...|(262144,[666,836,...|[1.0,0.0,0.0,0.0,...|       0.0|\n",
      "|0e180e3b047499fc0...|(262144,[5595,863...|[1.0,2.8535249672...|       0.0|\n",
      "|0ea1711e196d18f69...|(262144,[666,2625...|[5.96670201508172...|       1.0|\n",
      "|0f5e7ce476cbf3151...|(262144,[666,2026...|[1.0,7.7142063238...|       0.0|\n",
      "|0f64c1f859f34f397...|(262144,[2786,316...|[3.06073912837278...|       1.0|\n",
      "|0f9aaacbdad195c01...|(262144,[666,1466...|[1.33443242E-315,...|       2.0|\n",
      "|0ffbcddbc1c2527ec...|(262144,[666,5213...|[1.0,1.2697889629...|       0.0|\n",
      "|100247e9daacdc3d7...|(262144,[1202,335...|[0.0,0.0,0.0,0.0,...|       4.0|\n",
      "|101419557b4a7bd29...|(262144,[314,836,...|[1.0,4.0254535153...|       0.0|\n",
      "|104cfe0e69a51cf9f...|(262144,[29,1707,...|[1.0,0.0,0.0,0.0,...|       0.0|\n",
      "|10c8214c3c721e2a5...|(262144,[3890,108...|[1.0,1.2184645096...|       0.0|\n",
      "|11def60675b38fbb8...|(262144,[836,4578...|[4.10063108273522...|       1.0|\n",
      "|1296900b079ac8cec...|(262144,[1132,176...|[1.0,0.0,0.0,0.0,...|       0.0|\n",
      "|129954438d84b5a8a...|(262144,[836,1558...|[1.0,1.2994972324...|       0.0|\n",
      "|1344869f35e7f88e8...|(262144,[1251,170...|[1.0,0.0,0.0,0.0,...|       0.0|\n",
      "|1344f5ddce6692335...|(262144,[1707,183...|[0.0,0.0,1.0,0.0,...|       2.0|\n",
      "|13531a593bf382e8e...|(262144,[168,170,...|[0.0,0.0,0.0,1.0,...|       3.0|\n",
      "|13cc0633cce64b717...|(262144,[666,1707...|[1.0,0.0,0.0,0.0,...|       0.0|\n",
      "|1419276009d366a98...|(262144,[1251,278...|[1.0,7.9500981178...|       0.0|\n",
      "|14236b5e66e69bc21...|(262144,[1769,559...|[1.0,5.2957096523...|       0.0|\n",
      "|1434efe4d84173894...|(262144,[1736,383...|[0.0,0.0,1.0,0.0,...|       2.0|\n",
      "|1448bafe9ebc4a54e...|(262144,[666,836,...|[1.0,8.5255000685...|       0.0|\n",
      "|14ec024c18ac73f9f...|(262144,[3617,686...|[0.0,0.0,0.0,1.0,...|       3.0|\n",
      "|14f8df40543421f72...|(262144,[666,1707...|[1.16069760390404...|       1.0|\n",
      "|1511b88b9884cb5f0...|(262144,[1769,181...|[1.85254918642788...|       1.0|\n",
      "|15490223765233719...|(262144,[666,1156...|[1.0,0.0,0.0,0.0,...|       0.0|\n",
      "|15b880acd6533a81e...|(262144,[3483,366...|[1.0,4.9792964104...|       0.0|\n",
      "|164a051706af11e9b...|(262144,[1707,184...|[1.0,1.3044438043...|       0.0|\n",
      "|164e0798ecea69ef1...|(262144,[666,1769...|[8.12452681744952...|       1.0|\n",
      "|16989134ea5ebc0ad...|(262144,[666,1769...|[0.0,1.0,0.0,0.0,...|       1.0|\n",
      "|17afb24a0fd6f5d8b...|(262144,[666,1506...|[1.0,0.0,0.0,0.0,...|       0.0|\n",
      "|17e768b350bcdfe3a...|(262144,[3778,392...|[1.0,0.0,0.0,0.0,...|       0.0|\n",
      "|17fbe8328258b7e0d...|(262144,[666,1769...|[8.86207320110974...|       1.0|\n",
      "|181a7b7edf4f1e6ac...|(262144,[958,1652...|[0.0,0.0,0.0,1.0,...|       3.0|\n",
      "|18d0d8592fbdf1136...|(262144,[2437,420...|[1.0,8.6217392038...|       0.0|\n",
      "|190812e971ca26195...|(262144,[1667,176...|[1.0,1.3239673961...|       0.0|\n",
      "|1929fd1ac2a86652e...|(262144,[666,3009...|[1.0,4.4683561274...|       0.0|\n",
      "|1989bf8f3de607924...|(262144,[1769,688...|[0.99999999936658...|       0.0|\n",
      "|19ff77e874c0706f7...|(262144,[1769,252...|[3.07199548426839...|       2.0|\n",
      "|1a05c467efd5f48bf...|(262144,[1251,576...|[1.0,7.2576198366...|       0.0|\n",
      "|1a68a18c241891540...|(262144,[545,1769...|[1.05855006969267...|       2.0|\n",
      "|1afdb2fbad2a5cb21...|(262144,[5125,930...|[0.0,0.0,0.0,1.0,...|       3.0|\n",
      "|1b3f6e0bc46bcf400...|(262144,[170,666,...|[1.0,0.0,0.0,0.0,...|       0.0|\n",
      "|1bc80862be4a7ca8b...|(262144,[2710,287...|[1.0,0.0,0.0,0.0,...|       0.0|\n",
      "|1bed4f8a5412cfa31...|(262144,[666,1707...|[1.0,2.2691254380...|       0.0|\n",
      "|1c2588e384775cf0b...|(262144,[535,3824...|[1.0,0.0,0.0,0.0,...|       0.0|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_new.select('paper_id', 'features', 'probability','prediction').show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
